# Multi-Policy Cache System (C++ 高性能通用缓存系统)

这是一个基于 C++17 开发的轻量级、插件化缓存库。项目实现了从基础的 FIFO 到复杂的自适应算法 ARC 等多种策略，并提供了针对高并发场景的哈希分片优化。通过内置的基准测试工具，可以直观地对比各算法在不同工作负载下的命中率表现。

##  项目特性
- **泛型设计**：支持任意类型的 Key 和 Value。
- **模块化架构**：算法实现与基类解耦，易于扩展。
- **高并发支持**：提供基于分片锁（Sharding）的 Hash-LRU 和 Hash-LFU，降低锁竞争。
- **工业级算法**：包含 LRU-K 和 ARC 等能够有效对抗缓存污染的先进算法。

---

## 📂 目录结构说明

```text
myCacheProject/
├── src/
│   ├── Common/         # 公共基类 (CachePolicy) 与 核心数据结构
│   ├── FIFO/           # 先进先出算法
│   ├── LRU/            # LRU 相关 (包含标准 LRU, LRU-K, Hash-LRU)
│   ├── LFU/            # LFU 相关 (包含标准 LFU, Hash-LFU)
│   ├── ARC/            # 自适应缓存替换算法 (核心模块)
│   └── test.cpp        # 综合基准测试程序
└── CMakeLists.txt      # 自动化构建脚本

# 缓存置换算法实现

## 1. FIFO (First In First Out) - 先进先出
**源码实现**: `FIFOCache.hpp`

FIFO 是最直观的置换算法。它假设最早进入缓存的数据，其价值会随时间流逝而递减。

### 实现细节：
- **容器选择**：采用 `std::queue` 管理进入的时间序列，配合 `std::unordered_set` 提供 $O(1)$ 的命中查询。
- **置换逻辑**：当缓存达到容量限制时，直接从队首 `pop` 掉最老的页面。

### 优缺点：
- **优点**：实现极其简单，且没有 LRU 链表调整的开销，在数据访问模式非常均匀的场景下性能尚可。
- **缺点**：完全不考虑数据的“热度”，存在 Belady 异常（容量增加时命中率反而下降）。

## 2. LRU (Least Recently Used) - 最近最少使用
**源码实现**: `LRU.hpp`

基于“时间局部性”原理：如果数据最近被访问过，那么它在未来被访问的概率极高。

### 底层架构：
- **哈希链表**：通过 `unordered_map<Key, shared_ptr<Node>>` 定位，通过手动维护的双向链表实现 $O(1)$ 的节点移动。
- **内存管理**：在 `LRUNode` 中，`_prev` 使用 `std::weak_ptr`，`_next` 使用 `std::shared_ptr`。这是 C++ 内存管理的最佳实践，有效防止了双向链表中的循环引用（Circular Reference）导致的内存泄漏。

### 操作策略：
每次 `get` 命中或 `put` 更新，都会将节点原子性地移动到链表头部（Most Recently Used）。

## 3. LRU-K (Least Recently Used K) - 扫描抗性优化
**实现细节**：本项目在 LRU 逻辑基础上通过 `_accessCount` 扩展。

传统 LRU 对“偶发性扫描”极其脆弱（一次全表扫描会冲掉所有热点数据）。LRU-K 引入了“进入门槛”。

### 核心逻辑：
- **两阶段过滤**：数据首次进入时不直接放入核心缓存，而是放在历史队列。
- **晋升机制**：只有当 Key 被访问满 $K$ 次（本项目默认 $K=2$）后，才会被“晋升”至真正的缓存队列。

### 价值：
LRU-K 能有效识别“真热点”，避免偶尔被访问的数据污染主缓存。

## 4. Hash-Sharding (并发分片锁) - 并发瓶颈突破
**源码实现**: `HashLRU.hpp` / `HashLFUCache.hpp`

在高并发场景下，单锁实现的缓存会因为锁竞争（Lock Contention）导致多核 CPU 的吞吐量雪崩。

### 分片原理：
将一个大缓存逻辑拆分为 $N$ 个独立的小缓存分片（Slice）。

### 路由算法：
`Index = std::hash<Key>{}(key) % SliceNum`。

### 性能提升：
每个分片拥有独立的 `std::mutex`。这意味着在理想状态下，系统的并发处理能力提升了 $N$ 倍，锁冲突概率降低到原来的 $1/N$。

### 硬件适配：
默认自动获取 `std::thread::hardware_concurrency()`，确保在不同架构的服务器上都能达到最优分片配比。

## 5. LFU (Least Frequently Used) - 高效频率感知
**源码实现**: `LFUCache.hpp`

LFU 侧重于“访问频次”，适用于数据热度分布相对稳定的场景。

### 实现亮点 (O(1) 复杂度)：
- 不同于传统的 $O(\log N)$ 堆实现，本项目通过 `FreqList`（频率桶）实现。
- **结构**：`map<int, list<NodePtr>>`，Key 是访问次数，Value 是该频次下的所有节点。
- **最小频率指针**：维护 `_minFreq` 指针，使得淘汰时能瞬间定位到访问最少的节点。

### 老化机制 (Aging)：
为了防止某些数据在早期被高频访问后成为“永久钉子户”，引入了平均频率阈值。

### 逻辑：
当平均频次超过 `_maxAverageNum` 时，会对全局频次进行衰减处理，赋予新数据晋升的机会。

## 6. ARC (Adaptive Replacement Cache) - 自适应替换
**源码实现**: `ArcCache.hpp` (核心)

ARC 是本项目的工程巅峰。它结合了 LRU 和 LFU 的优点，并能根据工作负载实时自调整策略。

### 四列表动态架构：
- **T1 & B1 (LRU 及其幽灵)**：追踪最近抓取过一次的数据。
- **T2 & B2 (LFU 及其幽灵)**：追踪被多次访问的频率数据。

### 自适应学习机制：
- **命中 B1**：说明最近淘汰的数据其实很有用，算法通过 `increaseCapacity()` 自动增大 $T1$ 的比例 $p$。
- **命中 B2**：说明高频数据被踢出的太快，算法会增大 $T2$ 的配额。

### 优势：
ARC 在全表扫描、局部频繁访问、以及两者混合的场景下，命中率均能自动逼近理论最优值，且无需任何人工调参。

## 7. Common 基础设施 - 面向对象与多态
**源码实现**: `CachePolicy.hpp` / `ArcCacheNode.hpp`

### CachePolicy：
采用 Template Method 设计模式。定义了统一的虚接口 `put` 和 `get`。这使得 `test.cpp` 测试程序可以使用同一个基类指针指向不同的算法实例，实现公平的 Benchmark 对比。

### 智能指针深度应用：
项目中大量使用 `std::shared_ptr` 管理节点生命周期，利用 `std::unique_ptr` 管理分片实例。不仅简化了内存回收，还通过 `std::weak_ptr` 解决了复杂的双向链表状态迁移过程中的所有权问题。
